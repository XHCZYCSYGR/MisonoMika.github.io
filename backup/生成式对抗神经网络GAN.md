# 基本概念
## GAN的任务
     顾名思义，我们的目标是生成和输入X类似或具有相同特征的图像、文本等。
## ~~那我问你~~  GAN（Generative Adversarial Networks）与一般的神经网络有什么不同？
   一般的神经网络是“单一”的输入，即tensor[X]，但是GAN的输入是X+Z（类似通信原理的AWGN信道，只不过Z不一定是高斯分布），Z是已知简单分布的采样值（同样转为tensor），不一定和X同维，可以与X做不同的运算，且每次都会更新。然后整体作为输入。我们的目标是让GAN不仅注意到X，会受到Z的影响。
## 如何理解对抗？
     Generator（G）与Distribution（D）的对抗可以通俗的理解为：捕食者与被捕食者。对抗的过程大致如下：
- G.v1（初始化参数，随机的）对采样得到的Z进行学习，得到输出Y.v1
- 更新D.v1中的参数，使D.v1可以分辨Y.v1与输入X的差别。对D的理解：D的功能可以看作对Y.v1和X进行分类（或打分），D的输出可以是分类的Label（分数）
- 固定D.v1，更新G.v1的参数（Z也会变），使更新后G.v2的输出Y.v2可以“骗过”D.v1，即经D.v1判决或评分后，Y.v2和X没有区别
- 固定G.v2，更新D.v1的参数，使更新后D.v2可以分辨Y.v2和X的区别
- 循环上述步骤得到具有生成的G.vxxx

# GAN的理论支持
## GAN遇到的问题
我们要优化的目标函数：
    
$$
G^* = arg \  \underset{G}{min} \ Div(P_G \  , \  P_{data})
$$

其中， $P_G$表示经过Generator输出的Y的分布， $P_{data}$表示X的原始分布，Div是divergence的缩写。也就是说，我们的目标是找到一个最佳的G，使 $P_G$和 $P_{data}$ 的概率分布最为接近。
一般情况下， $Div(P_G \  , \  P_{data})$ 可以是KL散度或JS散度，但是，对于未知的 $P_{data}$，若按照定义式计算，可行性极差（学习途中偶遇超级积分，拼尽全力无法战胜）。
GAN的解决方案：**Sampling is GOOD enough**